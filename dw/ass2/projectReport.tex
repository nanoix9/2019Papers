% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  a4paper,
]{article}

\usepackage{etoolbox}
% \setmainfont{Linux Libertine O}
\AtBeginEnvironment{quote}{\it\small}

\usepackage[toc,page,header]{appendix}
% \renewcommand{\appendixpagename}{\centering Appendices111}

% \usepackage[colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=blue, anchorcolor=blue]{hyperref}

\usepackage[]{mathpazo}
\usepackage{setspace}
% \usepackage{lmodern,bm}
% \usepackage{unicode-math}
\usepackage{amssymb}
\usepackage{amsmath}
% \usepackage{amsfonts}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  % \usepackage{textcomp} % provide euro and other symbols
  % \usepackage{newtxtext,newtxmath}
  % \usepackage{mathptmx} \usepackage{tgtermes}
  % \usepackage{FiraSans} 
  % \usepackage[sfdefault]{FiraSans} \usepackage{FiraMono} \renewcommand*\familydefault{\sfdefault}
  % \usepackage{tgpagella}
  % \usepackage[utopia]{mathdesign}
  % \usepackage[math]{iwona}
  % \usepackage[sfdefault,scaled=.85]{FiraSans} \usepackage{newtxsf}
  % \usepackage{cmbright}
  % \usepackage{ccfonts,eulervm}  \usepackage[T1]{fontenc}
  % \usepackage[math]{kurier}
  % \usepackage{fourier}
  % \usepackage{mathpazo}
  % \usepackage{isomath}
  \else % if luatex or xetex
  % \usepackage{fourier}
  % \usepackage{mathpazo}
  % \usepackage{cmbright}
  % \usepackage{isomath}
  \usepackage{fontspec}
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
  % \setmainfont{Optima}
  % \setsansfont{Optima}
  % \usepackage{unicode-math}
  % \setmathfont{Optima}
  % \setmathfont{XITS Math}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdfauthor={Stone Fang (Student ID: 19049045)},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=25mm]{geometry}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\pagestyle{empty}

% \usepackage{ifthen}
% Using fancy headers and footers
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{} % clear all header fields
\fancyhead[L]{COMP810 Data Warehousing and Big Data Assessment 2}
% \fancyhead[CO,CE]{\ifthenelse{\value{page}=1}{first page}
% {COMP810 Data Warehousing and Big Data Assessment 2}}
\fancyfoot{} % clear all footer fields
\fancyfoot[LO,RE]{Stone Fang (19049045)}
\fancyfoot[LE,RO]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\renewcommand*{\thefootnote}{ [\arabic{footnote}]}

\fancypagestyle{plain}{\pagestyle{fancy}}
\usepackage[noabbrev]{cleveref}
    \usepackage{tcolorbox}

    \newcommand{\sqloutputbegin}[1][SQL Output]{
        \hspace{0.5cm}
        \begin{minipage}[c]{0.93\linewidth} 
        \centering
        \begin{tcolorbox}[colback=gray!8, colframe=white,
            boxrule=0pt, coltitle=black,
            colbacktitle=gray!15, title={#1}]
        \centering
        \small}
    \newcommand{\sqloutputend}{
        \end{tcolorbox} 
        \end{minipage}}
\usepackage[]{biblatex}
% redefine the booktitle macro or apacase directive to preserve the source casing for the booktitle field in the @inproceedings
\renewbibmacro*{booktitle}{\printfield{booktitle}}
% \DeclareFieldFormat{apacase}{%
%   \ifboolexpr{ test {\ifentrytype{inproceedings}}
%     and ( test {\ifcurrentfield{booktitle}}
%           or test {\ifcurrentfield{booksubtitle}} ) }
%     {#1}{\MakeSentenceCase*{#1}}}
\addbibresource{ass1.bib}

\title{    \textbf{COMP810 Data Warehousing and Big Data \\
    Assessment 2 Data Warehousing Project}}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Building and Analysing a DW for NatureFresh Stores in NZ}
\author{Stone Fang (Student ID: 19049045)}
\date{}

\begin{document}
\maketitle

\setstretch{1.25}
\hypertarget{project-overview}{%
\section{Project Overview}\label{project-overview}}

The goal of this project is to create a Data Warehouse (DW) for the
sales analysis of NatureFresh, one of the largest fresh food market
chains in New Zealand. Analysis of sales and customer shopping
behaviours can give NatureFresh in-depth insight of the market, so they
can improve their selling strategies accordingly.

The original available data are customer transactions and product
information. The transaction data contains records of customer buying,
including who (customer) bought what (product), when (date),
where(store) and how many was bought (quantity). The product data
contains information for each product, including supplier and price.

However, the format of original data doesn't fit into the requirement of
OLAP, so first we need transform the data into other formats for better
querying.

The major content of this project contains:

\begin{itemize}
\tightlist
\item
  Design and implement the star-schema for sales DW, i.e.~fact \&
  dimension tables
\item
  Fill DW by ETL process. Specifically, do Index Nested Loop Join (INLJ)
  on transactions and master data, transform and load data into fact \&
  dimension tables.
\item
  Execute queries on DW
\end{itemize}

All the operations above are implemented in SQL.

\hypertarget{schema-for-dw}{%
\section{Schema for DW}\label{schema-for-dw}}

According to the original data, the DW will consist of one fact table
\emph{Sales} and five dimension tables \emph{Product}, \emph{Supplier},
\emph{Customer}, \emph{Store}, and \emph{Date}, as shown in
\cref{fig:overall}. The SQL code to create all tables are in file
\emph{createDW.sql}.

\begin{figure}[htbp]
  \centering
  % \sffamily
  {
  %\fontfamily{phv}\selectfont
  \fontsize{9}{10}\selectfont
  % \fontsize{7}{7}\selectfont
  \def\svgwidth{0.9\columnwidth}
    \resizebox{0.9\textwidth}{!}{\input{star-schema.pdf_tex}}
  }
  \caption{Star Schema of NatureFresh Sales}
  \label{fig:overall}
\end{figure}

\hypertarget{fact-table}{%
\subsection{Fact Table}\label{fact-table}}

Apparently the fact table should have foreign keys corresponding to all
five dimension tables, and the quantity of item sold. There are two
decisions have been make for primary key and amount of money in sales.

\textbf{Primary key} of fact table can be a combination of all foreign
keys. However, there could be a concern to have more than one
transactions for the same values on all five dimensions. A quick
analysis shows that such situation does exists, though the possibility
is low. In other words, a customer may buy one product multiple times at
one store in one day. There are two options to solve this problem. One
is summing up the quantities of multiple transactions, resulting in only
one record for the same combination of dimension values. The other is
keep multiple transactions while use a separated ID field as the primary
key of \emph{Sales} fact table. In this project, the latter solution is
preferred because this approach can keep the original granularity of
transactions, thus contains more information. Also, the possibility of
multiple transactions for one combination of dimensions is low, so there
would not be significant overhead in terms of memory and storage.

\textbf{Price/Amount} is another concerning field. In the original data,
\emph{price} is stored in master data table as a property of product, so
it is natural to make it an attribute of product dimension. However,
this design has a shortcoming when price changes as it always does. If
the price of a product changes, we can't simply modify the value in
\emph{Product} dimension table otherwise the result on sales before that
change will be incorrect. Therefore, in this project price information
is kept in \emph{Sales} table. Since the amount of money in sales is a
more frequent used number, we add to fact table an \emph{amount} filed
which is calculated by \(\mathit{quantity}\times \mathit{price}\). In
\cref{price-attribute} further discussions will be provided on this
issue.

\hypertarget{dimensions}{%
\subsection{Dimensions}\label{dimensions}}

Details of dimension tables can be referred to \cref{fig:overall}. Most
dimensions are as simple as ``ID+name'', while the \emph{Date} dimension
is relatively complicated. First of all, unlike other dimensions, there
is no existing ID for \emph{Date}. In this project, a string in format
of ``YYYYMMDD'' is chosen as the ID for \emph{Date}, rather than an
auto-incremental column. The advantage is such ID is more readable and
intuitive, and thus more convenient for partitioning if required in the
future. On the other hand, it would need more storage space, which,
however, is not a big issue providing the cost storage is quite low
nowadays. Second, \emph{Date} dimension contains more information other
than names. In this project, common properties are calculated, including
\emph{year}, \emph{quarter}, \emph{month}, \emph{week}, \emph{day},
\emph{day\_of\_week}. In fact, it can be extended to more fields such as
\emph{is\_public\_holiday}, if some analysis on holiday is in demand.

\hypertarget{inlj-algorithm}{%
\section{INLJ Algorithm}\label{inlj-algorithm}}

Index Nested Loop Join (INLJ) is a table joining algorithm that can be
used for stream data joining. Nested Loop Join takes an outer loop and
an inner loop, each for one table, and output the rows that matches the
conditions, so the time complexity is \(O(N M)\) where \(N\) and \(M\)
are the number of rows of two tables. . However, INLJ only keep the
outer loop and replace the inner loop with an index-based loop up, thus
greatly reduce the time complexity. For example, if the index is
implemented by B-tree, then complexity of lookup is a logarithm of \(M\)
instead of linear which is the case of the inner loop.

This algorithm is implemented in PL/SQL. First, a bulk (50 rows in this
project) of transactions are read into memory. Then all rows in the bulk
are read one after another, and retrieve the information for current row
from master data by \emph{product\_id}. Then all properties
corresponding to current row are transformed to fit the star schema and
then load into the fact and dimension tables. Please refer to file
\emph{INLJ.sql} for the complete implementation.

\hypertarget{olap-queries-results}{%
\section{OLAP Queries Results}\label{olap-queries-results}}

This section summarise the results of required analysis. The SQL
statements for these queries are referred to file \emph{queriesDW.sql}.

\let\oldsubsection\thesubsection
\renewcommand*{\thesubsection}{Question~\arabic{subsection}}

\hypertarget{section}{%
\subsection{}\label{section}}

\begin{quote}
Determine the top 5 products in Dec 2019 in terms of total sales
\end{quote}

\sqloutputbegin

\begin{verbatim}
PRODUCT_NAME                   TOTAL_SALES       RANK
------------------------------ ----------- ----------
Bouillon cubes                     1759.58          1
Kiwis                              1757.75          2
Mac and cheese                        1632          3
Relish                             1574.18          4
Pears                              1396.53          5
\end{verbatim}

\sqloutputend

\hypertarget{section-1}{%
\subsection{}\label{section-1}}

\begin{quote}
Determine which store produced highest sales in the whole year?
\end{quote}

\sqloutputbegin

\begin{verbatim}
STORE_NAME           TOTAL_SALES       RANK
-------------------- ----------- ----------
Manukau                 82873.81          1
\end{verbatim}

\sqloutputend

\hypertarget{section-2}{%
\subsection{}\label{section-2}}

\begin{quote}
Determine the top 3 products for a month (say, Dec 2019), and for the 2
months before that, in terms of total sales.
\end{quote}

\sqloutputbegin

\begin{verbatim}
PRODUCT_NAME                   TOTAL_SALES       RANK      MONTH       YEAR
------------------------------ ----------- ---------- ---------- ----------
Bouillon cubes                     1759.58          1         12       2019
Kiwis                              1757.75          2         12       2019
Mac and cheese                        1632          3         12       2019
Onions                             2296.74          1         11       2019
Relish                             1751.91          2         11       2019
Broccoli                           1514.52          3         11       2019
Paprika                             1692.6          1         10       2019
Pizza / Pizza Rolls                   1505          2         10       2019
Oregano                             1476.8          3         10       2019
\end{verbatim}

\sqloutputend

\hypertarget{section-3}{%
\subsection{}\label{section-3}}

\begin{quote}
Create a materialised view called ``STOREANALYSIS'' that presents the
product-wise sales analysis for each store. The results should be
ordered by StoreID and then ProductID.
\end{quote}

\sqloutputbegin[SQL Output\footnotemark]

\begin{verbatim}
STOR PRODUC TOTAL_SALES
---- ------ -----------
S-1  P-1001       540.9
S-1  P-1002       164.4
S-1  P-1003      448.76
S-1  P-1004       250.2
S-1  P-1005     1318.68
\end{verbatim}

\sqloutputend

\footnotetext{
The output from Oracle SQL Developer doesn't show the complete names of columns,
which I assume is a feature on materilised view to save display areas.
It's only a sample query for inspection of the materialised view which has been 
just created. The content is correct, so the output was just copied and 
pasted here without any manual editing.
}

\hypertarget{section-4}{%
\subsection{}\label{section-4}}

\begin{quote}
Think about what information can be retrieved from the materialised view
created in Q4 using ROLLUP or CUBE concepts and provide some useful
information of your choice for management.
\end{quote}

\renewcommand*{\thesubsection}{\oldsubsection}

\hypertarget{discussion}{%
\section{Discussion}\label{discussion}}

\hypertarget{price-attribute}{%
\subsection{Price Attribute}\label{price-attribute}}

In this design, the price information is beared in the \emph{amount}
field of fact table because of the fact that prices of products are
subject to change. However, changes of prices are way less frequent than
transactions, so there will be much redundant storage for price
information. An alternative design is to add an extra dimension
\emph{SellingItem} which is simply a combination of \emph{Product} and
\emph{price}. When price changes, a new ``selling item'' will be created
with the same \emph{product\_id} and the new \emph{price}. This method
is show in \cref{fig:alter-price}. The benefit is reducing the required
storage for price by normalisation. However, this alternative design
ends up with an architecture other than star-schema.

\begin{figure}[htbp]
  \centering
  % \sffamily
  {
  %\fontfamily{phv}\selectfont
  \fontsize{10}{11}\selectfont
  % \fontsize{7}{7}\selectfont
  %\def\svgwidth{0.9\textwidth}
    \resizebox{0.85\textwidth}{!}{\input{alternative-price-attr.pdf_tex}}
  }
  \caption{Alternative Design for Price Attribute}
  \label{fig:alter-price}
\end{figure}

\hypertarget{summary-of-learning-outcomes}{%
\section{Summary of Learning
Outcomes}\label{summary-of-learning-outcomes}}

It is a great way to learn from developing a complete DW project. The
main outcomes learned from this project includes DW design, ETL process,
DW queries, and Oracle PL/SQL and SQL features.

\hypertarget{dw-development-lifecycle}{%
\subsection{DW Development Lifecycle}\label{dw-development-lifecycle}}

The first and foremost thing I learned is how to develop a DW through
the whole lifecycle. It involves requirement analysis, schema design,
data ETL, and querying. We need to divide this complex task into
different stages, and also be able to combine them together back into a
complete and running DW project.

\hypertarget{dw-schema}{%
\subsection{DW Schema}\label{dw-schema}}

The DW is usually modelled as data cube which consists of dimensions and
measurements. In terms of physical model, dimensions are mapped to
dimension tables and measurements are mapped to fact tables. There are
two major schema for DW design, star-schema and snowflake schema. In
this project we use star-schema, which is more efficient for querying
because it has less table joints for a query. However, the disadvantage
is the lack of normalisation. Most dimensions are simple, containing
only one attributed of ``name''. However, the \emph{Date} dimension is
trickier.

\hypertarget{inlj-etl}{%
\subsection{INLJ \& ETL}\label{inlj-etl}}

INLJ is suitable for joining stream data with batch data in near
real-time. The key idea of INLJ of two tables is looping on the first
table and looking up on the second table through an index. Apparently,
index cannot be build on the stream data, so we can only create index on
the batch data table and loop on the stream data. It is an effective way
to avoid exhaustive search on the batch table.

To implement INLJ in PL/SQL, ``cursor'' is an important tool. A cursor
fetches data from the output of a query, and can be traversed by a for
loop. Cursors can fetch data row by row or in bulk. The difference
between the two manner is performance. Each time when a cursor fetches
data, there will be context switching between PL/SQL engine and SQL
engine, which incurs overhead decreasing the overall performance.
Therefore, fetching the data bulk by bulk (e.g.~50 rows at one time) can
significantly increase the executing performance. To access the data
from a cursor we need to declare variables to hold that data. We can
either declare a variable in type of the whole row or multiple variables
each one for a certain field.

Before insert values into dimension tables, it is necessary to check
whether that value already exists in the table or not. There are
multiple ways to check the existance of one row before insertion. In
this project a \texttt{WHERE\ NOT\ EXISTS} clause is used along with
\texttt{INSERT\ INTO\ ...\ SELECT\ ...} statement to achieve this
purpose, but the trick here is the use of a dummy table \texttt{dual}.
The reason is as follows. The \texttt{INSERT\ INTO\ ...\ SELECT}
statement will select rows from a table and insert into the target
table, but here the data to insert actually comes from PL/SQL variables.
Therefore, the dummy table \texttt{dual} is put in the \texttt{FROM}
clause, but the columns in \texttt{SELECT} clause are actually the
variables. With this approach we can make use of
\texttt{INSERT\ INTO\ ...\ SELECT} statement to check the existance of a
row before inserting it.

\hypertarget{dw-query-analysis}{%
\subsection{DW Query \& Analysis}\label{dw-query-analysis}}

Analysis on DW should be translated to SQL queries. It's usually
required to do table join so the query would be a bit complicated even
for a simple analysis.

\textbf{Top K retrieval} is a type of common analysis, which finds the
most or least measurements with corresponding dimension values. For
example, it's useful for decision-making in business operation to know
the products with K highest or lowest sales. This can be done by either
\texttt{ORDER\ BY} or \texttt{RANK()} operations in Oracle SQL
Developer. More than that, the analytics are usually done with
roll-up/drill-down/slice/dice operations. For instance, it's common to
aggregate the data of each day to month level (roll-up), or retrieve
data of a certain month (slice) or a month range (dice). For this
purpose, the \texttt{WHERE} and \texttt{GROUP\ BY} clause are used,
sometimes with \texttt{PARTITION\ BY} operation for advanced query.

\textbf{Dynamic query} is an important way to improve flexibility and
query reuse. Dynamic query is constructed in runtime with parameters
substituted by real values assigned to them. Next time if we want to do
similar analysis, we just need to change the parameter rather than
manually editing the query statement. To implement a dynamic query is
basically to create a PL/SQL table function with the query wrapped
inside it. It's called ``table function'' because it returns collections
of objects that minic tables. However, the details are somewhat
complicated. What I learned from developing a dynamic query includes:

\begin{itemize}
\tightlist
\item
  Create types of expected table for return type declaration in function
  by \texttt{CREATE\ TYPE}.
\item
  In side the function, create a parameterised cursor for data fetching.
  The parameters are about the target month, so the query can be used to
  select data of any specified month.
\item
  Use \texttt{FOR\ LOOP} and \texttt{IF\ THEN\ ELSE} to iteractively
  fetch data from database, and convert the retrieved data into the type
  matching the function declaration.
\item
  Use \texttt{PIPELINED} and \texttt{PIPE\ ROW} together to return the
  fetched records in an convenient way. Without this approach we would
  have to create a local collection, append records to it and return it
  after all data has been retrieved, but with \texttt{PIPELINED} we can
  simply ``pipe'' out each record immediately after it's retrieved
  without the needs for local collection.
\end{itemize}

\printbibliography

\end{document}

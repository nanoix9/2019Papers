@techreport{isoiec25010,
  type = {},
  groups = {public},
  publisher = {ISO/IEC 25010},
  timestamp = {2013-11-05T11:06:11.000+0100},
  title = {Systems and Software Engineering—Systems and Software Quality Requirements and Evaluation ({SQuaRE})—System and Software Quality Models},
  username = {},
  year = 2011
}

@INPROCEEDINGS{vliet2008km, 
  author={H. v. {Vliet}}, 
  booktitle={19th Australian Conference on Software Engineering (aswec 2008)}, 
  title={Software Architecture Knowledge Management}, 
  year={2008}, 
  volume={}, 
  number={}, 
  pages={24-31}, 
  keywords={knowledge management;software architecture;software architecture;knowledge management;system development;architectural knowledge;system evolution;Software architecture;Knowledge management;Connectors;Computer architecture;Road transportation;Software engineering;Computer science;Computer industry;Wiring;Finance;software architecture;knowledge management}, 
  doi={10.1109/ASWEC.2008.4483186}, 
  ISSN={1530-0803}, 
  month={March}
}

@ARTICLE{liu2018scholar_data_visualization,
author={J. {Liu} and T. {Tang} and W. {Wang} and B. {Xu} and X. {Kong} and F. {Xia}},
journal={IEEE Access},
title={A Survey of Scholarly Data Visualization},
year={2018},
volume={6},
number={},
pages={19205-19221},
abstract={Scholarly information usually contains millions of raw data, such as authors, papers, citations, as well as scholarly networks. With the rapid growth of the digital publishing and harvesting, how to visually present the data efficiently becomes challenging. Nowadays, various visualization techniques can be easily applied on scholarly data visualization and visual analysis, which enables scientists to have a better way to represent the structure of scholarly data sets and reveal hidden patterns in the data. In this paper, we first introduce the basic concepts and the collection of scholarly data. Then, we provide a comprehensive overview of related data visualization tools, existing techniques, as well as systems for the analyzing volumes of diverse scholarly data. Finally, open issues are discussed to pursue new solutions for abundant and complicated scholarly data visualization, as well as techniques, that support a multitude of facets.},
keywords={data visualisation;educational computing;electronic publishing;public domain software;scholarly information;raw data;scholarly networks;visualization techniques;visual analysis;scholarly data sets;diverse scholarly data;abundant data visualization;complicated scholarly data visualization;data visualization tools;Data visualization;Data mining;Tools;Data analysis;Libraries;Visual analytics;Big Data;Scholarly data;scholarly data analysis;scholarly data visualization;visual analysis},
doi={10.1109/ACCESS.2018.2815030},
ISSN={},
month={},}

@Article{beel2016:paper_recommender,
author="Beel, Joeran
and Gipp, Bela
and Langer, Stefan
and Breitinger, Corinna",
title="Research-paper recommender systems: a literature survey",
journal="International Journal on Digital Libraries",
year="2016",
month="Nov",
day="01",
volume="17",
number="4",
pages="305--338",
abstract={In the last 16 years, more than 200 research articles were published about research-paper recommender systems. We reviewed these articles and present some descriptive statistics in this paper, as well as a discussion about the major advancements and shortcomings and an overview of the most common recommendation concepts and approaches. We found that more than half of the recommendation approaches applied content-based filtering (55 {\%}). Collaborative filtering was applied by only 18 {\%} of the reviewed approaches, and graph-based recommendations by 16 {\%}. Other recommendation concepts included stereotyping, item-centric recommendations, and hybrid recommendations. The content-based filtering approaches mainly utilized papers that the users had authored, tagged, browsed, or downloaded. TF-IDF was the most frequently applied weighting scheme. In addition to simple terms, n-grams, topics, and citations were utilized to model users' information needs. Our review revealed some shortcomings of the current research. First, it remains unclear which recommendation concepts and approaches are the most promising. For instance, researchers reported different results on the performance of content-based and collaborative filtering. Sometimes content-based filtering performed better than collaborative filtering and sometimes it performed worse. We identified three potential reasons for the ambiguity of the results. (A) Several evaluations had limitations. They were based on strongly pruned datasets, few participants in user studies, or did not use appropriate baselines. (B) Some authors provided little information about their algorithms, which makes it difficult to re-implement the approaches. Consequently, researchers use different implementations of the same recommendations approaches, which might lead to variations in the results. (C) We speculated that minor variations in datasets, algorithms, or user populations inevitably lead to strong variations in the performance of the approaches. Hence, finding the most promising approaches is a challenge. As a second limitation, we noted that many authors neglected to take into account factors other than accuracy, for example overall user satisfaction. In addition, most approaches (81 {\%}) neglected the user-modeling process and did not infer information automatically but let users provide keywords, text snippets, or a single paper as input. Information on runtime was provided for 10 {\%} of the approaches. Finally, few research papers had an impact on research-paper recommender systems in practice. We also identified a lack of authority and long-term research interest in the field: 73 {\%} of the authors published no more than one paper on research-paper recommender systems, and there was little cooperation among different co-author groups. We concluded that several actions could improve the research landscape: developing a common evaluation framework, agreement on the information to include in research papers, a stronger focus on non-accuracy aspects and user modeling, a platform for researchers to exchange information, and an open-source framework that bundles the available recommendation approaches.},
issn="1432-1300",
doi="10.1007/s00799-015-0156-0",
url="https://doi.org/10.1007/s00799-015-0156-0"
}

@article{Lupiani-Ruiz:2011:Financial_semantic_search,
 author = {Lupiani-Ruiz, Eduardo and Garc\'{\i}A-Manotas, Ignacio and Valencia-Garc\'{\i}A, Rafael and Garc\'{\i}A-S\'{a}Nchez, Francisco and Castellanos-Nieves, Dagoberto and Fern\'{a}Ndez-Breis, Jesualdo Tom\'{a}S and Cam\'{o}N-Herrero, Juan Bosco},
 title = {Financial News Semantic Search Engine},
 journal = {Expert Syst. Appl.},
 issue_date = {November, 2011},
 volume = {38},
 number = {12},
 month = nov,
 year = {2011},
 issn = {0957-4174},
 pages = {15565--15572},
 numpages = {8},
 url = {http://dx.doi.org/10.1016/j.eswa.2011.06.003},
 doi = {10.1016/j.eswa.2011.06.003},
 acmid = {2286840},
 publisher = {Pergamon Press, Inc.},
 address = {Tarrytown, NY, USA},
 keywords = {Ontologies, Ontology population, Semantic Web, Semantic search engine},
} 

@INPROCEEDINGS{zhao:2017:kg,
author={X. {Zhao} and Z. {Xing} and M. A. {Kabir} and N. {Sawada} and J. {Li} and S. {Lin}},
booktitle={2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)},
title={HDSKG: Harvesting domain specific knowledge graph from content of webpages},
year={2017},
volume={},
number={},
pages={56-67},
abstract={Knowledge graph is useful for many different domains like search result ranking, recommendation, exploratory search, etc. It integrates structural information of concepts across multiple information sources, and links these concepts together. The extraction of domain specific relation triples (subject, verb phrase, object) is one of the important techniques for domain specific knowledge graph construction. In this research, an automatic method named HDSKG is proposed to discover domain specific concepts and their relation triples from the content of webpages. We incorporate the dependency parser with rule-based method to chunk the relations triple candidates, then we extract advanced features of these candidate relation triples to estimate the domain relevance by a machine learning algorithm. For the evaluation of our method, we apply HDSKG to Stack Overflow (a Q&A website about computer programming). As a result, we construct a knowledge graph of software engineering domain with 35279 relation triples, 44800 concepts, and 9660 unique verb phrases. The experimental results show that both the precision and recall of HDSKG (0.78 and 0.7 respectively) is much higher than the openIE (0.11 and 0.6 respectively). The performance is particularly efficient in the case of complex sentences. Further more, with the self-training technique we used in the classifier, HDSKG can be applied to other domain easily with less training data.},
keywords={estimation theory;grammars;graph theory;information retrieval;Internet;knowledge based systems;learning (artificial intelligence);software engineering;HDSKG;harvesting domain specific knowledge graph;Web page content;domain specific relation triple extraction;dependency parser;rule-based method;domain relevance estimation;machine learning algorithm;Stack Overflow;software engineering;Knowledge engineering;Feature extraction;Libraries;Data mining;Software engineering;Information retrieval;Support vector machines;Knowledge Graph;Structural Information Extraction;openIE;Stack Overflow;Dependency Parse},
doi={10.1109/SANER.2017.7884609},
ISSN={},
month={Feb},}

@InProceedings{lei:2006:semsearch,
author="Lei, Yuangui
and Uren, Victoria
and Motta, Enrico",
editor={Staab, Steffen
and Sv{\'a}tek, Vojt{\v{e}}ch},
title="SemSearch: A Search Engine for the Semantic Web",
booktitle="Managing Knowledge in a World of Networks",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="238--245",
abstract="Existing semantic search tools have been primarily designed to enhance the performance of traditional search technologies but with little support for ordinary end users who are not necessarily familiar with domain specific semantic data, ontologies, or SQL-like query languages. This paper presents SemSearch, a search engine, which pays special attention to this issue by providing several means to hide the complexity of semantic search from end users and thus make it easy to use and effective.",
isbn="978-3-540-46365-8"
}

@INPROCEEDINGS{beel:2017:mr_dlib,
author={J. {Beel} and A. {Aizawa} and C. {Breitinger} and B. {Gipp}},
booktitle={2017 ACM/IEEE Joint Conference on Digital Libraries (JCDL)},
title={Mr. DLib: Recommendations-as-a-Service (RaaS) for Academia},
year={2017},
volume={},
number={},
pages={1-2},
abstract={Only few digital libraries and reference managers offer recommender systems, although such systems could assist users facing information overload. In this paper, we introduce Mr. DLib's recommendations-as-a-service, which allows third parties to easily integrate a recommender system into their products. We explain the recommender approaches implemented in Mr. DLib (content-based filtering among others), and present details on 57 million recommendations, which Mr. DLib delivered to its partner GESIS Sowiport. Finally, we outline our plans for future development, including integration into JabRef, establishing a living lab, and providing personalized recommendations.},
keywords={academic libraries;collaborative filtering;content-based retrieval;digital libraries;recommender systems;recommendations-as-a-service;RaaS;Academia;digital libraries;reference managers;recommender systems;information overload;Mr. DLib;content-based filtering;GESIS Sowiport;JabRef;living lab;personalized recommendations;Recommender systems;Libraries;Conferences;Web sites;Bibliometrics;Web services},
doi={10.1109/JCDL.2017.7991606},
ISSN={},
month={June},}

@InProceedings{garcia:2006:component_search,
author={Garcia, Vinicius Cardoso
and Lucr{\'e}dio, Daniel
and Dur{\~a}o, Frederico Araujo
and Santos, Eduardo Cruz Reis
and de Almeida, Eduardo Santana
and de Mattos Fortes, Renata Pontin
and de Lemos Meira, Silvio Romero},
editor={Gorton, Ian
and Heineman, George T.
and Crnkovi{\'{c}}, Ivica
and Schmidt, Heinz W.
and Stafford, Judith A.
and Szyperski, Clemens
and Wallnau, Kurt},
title="From Specification to Experimentation: A Software Component Search Engine Architecture",
booktitle="Component-Based Software Engineering",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="82--97",
abstract="This paper presents a software component search engine, from the early specification and design steps to two experiments performed to evaluate its performance. After the experience gained from the use of this first version, several improvements were introduced. The current version of the engine combines text mining and facet-based search. The experiments indicated, so far, that using these two techniques together is better than using them separately. From the experience obtained in these experiments and in industrial tests, we point out possible improvements and future research directions, which are presented and discussed at the end of the paper.",
isbn="978-3-540-35629-5"
}

@ARTICLE{tuarob:2016:algorithm_search,
author={S. {Tuarob} and S. {Bhatia} and P. {Mitra} and C. L. {Giles}},
journal={IEEE Transactions on Big Data},
title={AlgorithmSeer: A System for Extracting and Searching for Algorithms in Scholarly Big Data},
year={2016},
volume={2},
number={1},
pages={3-17},
abstract={Algorithms are usually published in scholarly articles, especially in the computational sciences and related disciplines. The ability to automatically find and extract these algorithms in this increasingly vast collection of scholarly digital documents would enable algorithm indexing, searching, discovery, and analysis. Recently, AlgorithmSeer, a search engine for algorithms, has been investigated as part of CiteSeer' with the intent of providing a large algorithm database. Currently, over 200,000 algorithms have been extracted from over 2 million scholarly documents. This paper proposes a novel set of scalable techniques used by AlgorithmSeer to identify and extract algorithm representations in a heterogeneous pool of scholarly documents. Specifically, hybrid machine learning approaches are proposed to discover algorithm representations. Then, techniques to extract textual metadata for each algorithm are discussed. Finally, a demonstration version of AlgorithmSeer that is built on Solr/Lucene open source indexing and search system is presented.},
keywords={Big Data;database indexing;document handling;learning (artificial intelligence);meta data;public domain software;search engines;AlgorithmSeer;scholarly digital documents;algorithm indexing;algorithm searching;algorithm discovery;algorithm analysis;search engine;CiteSeer;large algorithm database;algorithm representation extraction;algorithm representation identification;hybrid machine learning approach;textual metadata extraction;Solr/Lucene open source indexing and search system;Machine learning algorithms;Search engines;Algorithm design and analysis;Big data;Software algorithms;Metadata;Data mining;Algorithm search engine;ensemble machine learning;scholarly big data;Algorithm search engine;ensemble machine learning;scholarly big data},
doi={10.1109/TBDATA.2016.2546302},
ISSN={},
month={March},}

@article{AMPATZOGLOU:2013:design_pattern,
title = "Building and mining a repository of design pattern instances: Practical and research benefits",
journal = "Entertainment Computing",
volume = "4",
number = "2",
pages = "131 - 142",
year = "2013",
issn = "1875-9521",
doi = "https://doi.org/10.1016/j.entcom.2012.10.002",
url = "http://www.sciencedirect.com/science/article/pii/S1875952112000195",
author = "Apostolos Ampatzoglou and Olia Michou and Ioannis Stamelos",
keywords = "Software engineering, Computer games, Design patterns, Repository",
abstract = "Design patterns are well-known design solutions that are reported to produce substantial benefits with respect to software quality. However, to our knowledge there are no scientific efforts on gathering information on software projects that use design patterns. This paper introduces a web repository of design patterns instances that have been used in open source projects. The usefulness of such a repository lies in the provision of a base of knowledge, where developers can identify reusable components and researchers can find a mined data set. Currently, 141 open source projects have been considered and more than 4500 pattern instances have been found and recorded in the database of the repository. The evaluation of the repository has been performed from an academic and a practical point of view. The results suggest that the repository can be useful for both experienced and inexperienced users. However, the benefits of using the repository are more significant for inexperienced users."
}

@ARTICLE{Allahyari:2017:text_mining_survey,
       author = {{Allahyari}, Mehdi and {Pouriyeh}, Seyedamin and {Assefi}, Mehdi and
         {Safaei}, Saied and {Trippe}, Elizabeth D. and {Gutierrez}, Juan B. and
         {Kochut}, Krys},
        title = "{A Brief Survey of Text Mining: Classification, Clustering and Extraction Techniques}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval},
         year = "2017",
        month = "Jul",
          eid = {arXiv:1707.02919},
        pages = {arXiv:1707.02919},
archivePrefix = {arXiv},
       eprint = {1707.02919},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170702919A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{RISTOSKI:2016:semantic_web_survey,
title = "Semantic Web in data mining and knowledge discovery: A comprehensive survey",
journal = "Journal of Web Semantics",
volume = "36",
pages = "1 - 22",
year = "2016",
issn = "1570-8268",
doi = "https://doi.org/10.1016/j.websem.2016.01.001",
url = "http://www.sciencedirect.com/science/article/pii/S1570826816000020",
author = "Petar Ristoski and Heiko Paulheim",
keywords = "Linked Open Data, Semantic Web, Data mining, Knowledge discovery",
abstract = "Data Mining and Knowledge Discovery in Databases (KDD) is a research field concerned with deriving higher-level insights from data. The tasks performed in that field are knowledge intensive and can often benefit from using additional knowledge from various sources. Therefore, many approaches have been proposed in this area that combine Semantic Web data with the data mining and knowledge discovery process. This survey article gives a comprehensive overview of those approaches in different stages of the knowledge discovery process. As an example, we show how Linked Open Data can be used at various stages for building content-based recommender systems. The survey shows that, while there are numerous interesting research works performed, the full potential of the Semantic Web and Linked Open Data for data mining and KDD is still to be unlocked."
}

@article{molloy:2011:open_knowledge,
author = {Molloy, Jennifer},
year = {2011},
month = {12},
pages = {e1001195},
title = {The Open Knowledge Foundation: Open Data Means Better Science},
volume = {9},
journal = {PLoS biology},
doi = {10.1371/journal.pbio.1001195}
}

@ARTICLE{kruchten:1995:4plus1view,
author={P. B. {Kruchten}},
journal={IEEE Software},
title={The 4+1 View Model of architecture},
year={1995},
volume={12},
number={6},
pages={42-50},
abstract={The 4+1 View Model organizes a description of a software architecture using five concurrent views, each of which addresses a specific set of concerns. Architects capture their design decisions in four views and use the fifth view to illustrate and validate them. The logical view describes the design's object model when an object-oriented design method is used. To design an application that is very data driven, you can use an alternative approach to develop some other form of logical view, such as an entity-relationship diagram. The process view describes the design's concurrency and synchronization aspects. The physical view describes the mapping of the software onto the hardware and reflects its distributed aspect. The development view describes the software's static organization in its development environment.<>},
keywords={software engineering;entity-relationship modelling;object-oriented methods;distributed processing;4+1 View Model;software architecture;design decisions;validation;design object model;object-oriented design method;data-driven application design;entity-relationship diagram;concurrency;synchronization;physical view;software mapping;hardware;distributed aspect;development view;software static organization;development environment;Computer architecture;Software architecture;Design methodology;Scalability;Physics computing;Data engineering;Application software;Concurrent computing;Hardware;Software design},
doi={10.1109/52.469759},
ISSN={},
month={Nov},}

@book{Bass:2012:SA_in_practice,
 author = {Bass, Len and Clements, Paul and Kazman, Rick},
 title = {Software Architecture in Practice},
 year = {2012},
 isbn = {0321815734, 9780321815736},
 edition = {3rd},
 publisher = {Addison-Wesley Professional},
} 

@techreport{Cook2017:uml,
  author = {Steve Cook and Conrad Bock and Pete Rivett and Tom Rutt and Ed Seidewitz and Bran Selic and Doug Tolbert},
  title = {Unified Modeling Language ({UML}) Version 2.5.1},
  institution = {Object Management Group ({OMG})},
  type = {Standard},
  month = Dec,
  year = {2017},
  url = {https://www.omg.org/spec/UML/2.5.1}
}
